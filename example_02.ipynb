{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# from pyhanlp import *\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer,BertForSequenceClassification,AdamW\n",
    "from torch.utils.data import TensorDataset,RandomSampler,DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据集\n",
    "train_dataset = 'data/train.csv'\n",
    "test_dataset = 'data/dev_id.csv'\n",
    "        \n",
    "def DataPrepare(file_name):\n",
    "    data = pd.read_csv(file_name)\n",
    "    data1 = pd.concat([data.question1,data.category],axis=1,keys=['question','label'])\n",
    "    data2 = pd.concat([data.question2,data.category],axis=1,keys=['question','label'])\n",
    "    data = pd.concat([data1,data2],axis=0,ignore_index=True)\n",
    "    def labels_ch(x):\n",
    "        if x == 'hypertension':\n",
    "            x = 0\n",
    "        if x == 'hepatitis':\n",
    "            x = 1\n",
    "        if x =='breast_cancer':\n",
    "            x = 2\n",
    "        if x=='aids':\n",
    "            x = 3\n",
    "        if x =='diabetes':\n",
    "            x = 4\n",
    "        return x\n",
    "    data.label = [labels_ch(la) for la in data.label]\n",
    "    return data\n",
    "\n",
    "train_dataset = DataPrepare(train_dataset)\n",
    "test_dataset = DataPrepare(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_seq_length:20(max_seq_length训练集最大值75),当前修改为20\n",
      "max_seq_length:20(max_seq_length训练集最大值75),当前修改为20\n"
     ]
    }
   ],
   "source": [
    "def data2feature(data):\n",
    "    # Load pretrained model/tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-chinese',unk_token='[UNK]')\n",
    "    # tokenizer = BertTokenizer.from_pretrained('./directory/save/')  # re-load\n",
    "\n",
    "    # Encode text\n",
    "    input_ids = {}\n",
    "    max_seq_length = 0\n",
    "    for i,sen in enumerate(data):\n",
    "        # Tokenized input,添加[CLS], [SEP]，并将 token 转为 vocabulary 索引\n",
    "        input_ids[i] = tokenizer.encode(sen, add_special_tokens=True,max_length=20)  \n",
    "        if max_seq_length < len(input_ids[i]): max_seq_length = len(input_ids[i])\n",
    "    # unsqueeze(0)   \n",
    "    print(\"max_seq_length:\"+str(max_seq_length)+\"(max_seq_length训练集最大值75),当前修改为20\")\n",
    "    max_seq_length = 20\n",
    "    # Zero-pad up to the sequence length.\n",
    "    def padding(input_ids,max_seq_length):\n",
    "        while len(input_ids) < max_seq_length:\n",
    "            input_ids.append(0)\n",
    "            #  input_mask.append(0)\n",
    "            #  segment_ids.append(0)\n",
    "        return input_ids\n",
    "    input = []\n",
    "    for i,_ in enumerate(input_ids):\n",
    "        input_ids[i] = padding(input_ids[i],max_seq_length)\n",
    "        input.append(input_ids[i])\n",
    "    input_ids = torch.LongTensor(input)\n",
    "    # tokenizer.save_pretrained('./directory/save/')  # save\n",
    "    # tokenizer = BertTokenizer.from_pretrained('./directory/save/')  # re-load\n",
    "    return input_ids \n",
    "\n",
    "#训练集\n",
    "train_inputs = data2feature(train_dataset.question)[0:36]\n",
    "train_labels = torch.tensor(train_dataset.label).reshape(-1,1)[0:36]  # Batch size 1\n",
    "\n",
    "#测试集\n",
    "test_inputs = data2feature(test_dataset.question)[0:36]\n",
    "test_labels = torch.tensor(test_dataset.label).reshape(-1,1)[0:36]  # Batch size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 因为取了两条数据，数据中label未按顺序填充，修改label\n",
    "# train_labels[0][0]=0\n",
    "# train_labels[1][0]=1\n",
    "# train_labels[2][0]=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成dataloader\n",
    "batch_size = 5\n",
    "train_data = TensorDataset(train_inputs, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
    "test_data = TensorDataset(test_inputs, test_labels)\n",
    "testtest_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "# print(\"train_inputs.shape()\",train_inputs.shape)  # train_inputs.shape() torch.Size([3, 20])\n",
    "# print(\"train_labels.shape()\",train_labels.shape)  # train_labels.shape() torch.Size([3, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-chinese',num_labels=5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(train_inputs, labels=train_labels)\n",
    "loss, logits = outputs[:5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2837, -0.3541,  0.1774,  0.8675,  0.2638],\n",
       "        [ 0.0450, -0.4117,  0.5731,  0.7860,  0.4932],\n",
       "        [ 0.3496, -0.1941,  0.3044,  0.8262, -0.0797],\n",
       "        [ 0.1567, -0.3271,  0.2776,  0.8463,  0.2944],\n",
       "        [ 0.1246, -0.4623,  0.5355,  0.6910,  0.3097],\n",
       "        [ 0.2761, -0.5173,  0.2268,  0.4589,  0.4879],\n",
       "        [ 0.2309, -0.3387,  0.2222,  0.5878,  0.2339],\n",
       "        [ 0.1289, -0.2991,  0.2546,  0.6653,  0.2744],\n",
       "        [ 0.2402, -0.4049,  0.3141,  0.7018,  0.3727],\n",
       "        [-0.0131, -0.4350,  0.5031,  0.7122,  0.3124],\n",
       "        [ 0.2771, -0.4721,  0.0620,  0.4655,  0.5370],\n",
       "        [ 0.3839, -0.4130, -0.0105,  0.5837,  0.2695],\n",
       "        [ 0.2063, -0.4219,  0.1917,  0.6721,  0.2647],\n",
       "        [ 0.0145, -0.3661,  0.4515,  0.8103,  0.4419],\n",
       "        [ 0.2349, -0.4967,  0.3600,  0.8605,  0.1619],\n",
       "        [ 0.0855, -0.3589,  0.3730,  0.8709,  0.4646],\n",
       "        [ 0.0685, -0.3638,  0.2832,  0.8250,  0.4209],\n",
       "        [ 0.2662, -0.5054,  0.0727,  0.6761,  0.3440],\n",
       "        [ 0.1818, -0.3803,  0.3391,  0.5770,  0.4893],\n",
       "        [ 0.0214, -0.3838,  0.4534,  0.8654,  0.3503],\n",
       "        [ 0.1699, -0.3035,  0.3573,  0.7527,  0.1764],\n",
       "        [ 0.2874, -0.4090,  0.0921,  0.5285,  0.4944],\n",
       "        [ 0.2935, -0.4233,  0.1501,  0.6554,  0.3297],\n",
       "        [ 0.3414, -0.3795,  0.2061,  0.6861,  0.2560],\n",
       "        [ 0.1705, -0.1069,  0.2778,  0.7152,  0.4496],\n",
       "        [ 0.1209, -0.3242,  0.3286,  0.7600,  0.3097],\n",
       "        [ 0.1784, -0.3753,  0.3335,  0.5909,  0.2773],\n",
       "        [ 0.3149, -0.2999,  0.0519,  0.4280,  0.5929],\n",
       "        [ 0.1164, -0.3625,  0.3691,  0.7423,  0.1513],\n",
       "        [ 0.1834, -0.2404,  0.4377,  0.4103,  0.4383],\n",
       "        [ 0.1759, -0.5189,  0.4627,  0.7534,  0.3335],\n",
       "        [ 0.2273, -0.3667,  0.2853,  0.8640,  0.3030],\n",
       "        [ 0.2077, -0.2535,  0.3931,  0.6986,  0.2195],\n",
       "        [ 0.1705, -0.1069,  0.2778,  0.7152,  0.4496],\n",
       "        [ 0.2169, -0.4935,  0.1174,  0.6250,  0.2262],\n",
       "        [ 0.1434, -0.2873,  0.1573,  0.6514,  0.0482]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(params=optimizer_grouped_parameters, \n",
    "                              lr=2e-5,correct_bias=False)\n",
    "# optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "#                      lr=2e-5,\n",
    "#                      warmup=.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义一个计算准确率的函数\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Train loss: 0.10502016427926719\n",
      "Test Accuracy: 0.7999999999999999\n",
      "Epoch: 1\n",
      "Train loss: 0.1546789885032922\n",
      "Test Accuracy: 0.95\n",
      "Epoch: 2\n",
      "Train loss: 0.07703826483339071\n",
      "Test Accuracy: 0.9249999999999999\n",
      "Epoch: 3\n",
      "Train loss: 0.01590970327379182\n",
      "Test Accuracy: 0.9249999999999999\n",
      "Epoch: 4\n",
      "Train loss: 0.007465500006219372\n",
      "Test Accuracy: 0.8999999999999999\n",
      "Epoch: 5\n",
      "Train loss: 0.0039281596400542185\n",
      "Test Accuracy: 0.8999999999999999\n",
      "Epoch: 6\n",
      "Train loss: 0.0024813308991724625\n",
      "Test Accuracy: 0.8999999999999999\n",
      "Epoch: 7\n",
      "Train loss: 0.002139362259185873\n",
      "Test Accuracy: 0.8999999999999999\n",
      "Epoch: 8\n",
      "Train loss: 0.001958146160177421\n",
      "Test Accuracy: 0.8999999999999999\n",
      "Epoch: 9\n",
      "Train loss: 0.0017257512590731494\n",
      "Test Accuracy: 0.8999999999999999\n"
     ]
    }
   ],
   "source": [
    "#训练开始\n",
    "train_loss_set = []#可以将loss加入到列表中，后期画图使用\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch: {}\".format(epoch))\n",
    "    #训练开始\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        #取第一个位置，BertForSequenceClassification第一个位置是Loss，第二个位置是[CLS]的logits\n",
    "        loss = model(b_input_ids, token_type_ids=None, labels=b_labels)[0]\n",
    "        train_loss_set.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "    print(\"Train loss: {}\".format(tr_loss / nb_tr_steps))\n",
    "    #模型评估\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    for batch in test_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, token_type_ids=None)[0]\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += 1\n",
    "    print(\"Test Accuracy: {}\".format(eval_accuracy / nb_eval_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_inputs[0]: torch.Size([1, 20])\n",
      "test_labels[0]: tensor([[2],\n",
      "        [3]])\n"
     ]
    }
   ],
   "source": [
    "print(\"test_inputs[0]:\",test_inputs[0].reshape(-1,20).shape) \n",
    "print(\"test_labels[0]:\",test_labels[0:2])\n",
    "logits = model(test_inputs[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.6879, -1.6426,  4.6078, -1.7010, -0.7756],\n",
       "         [-1.0576, -0.4844, -0.8272,  6.0423, -1.0487]],\n",
       "        grad_fn=<AddmmBackward>),)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained('./directory/save/')  # save\n",
    "\n",
    "#改在训练的时候，判断准确最高的保存 \n",
    "\n",
    "# model = model_class.from_pretrained('./directory/save/')  # re-load\n",
    "# tokenizer.save_pretrained('./directory/save/')  # save\n",
    "# tokenizer = BertTokenizer.from_pretrained('./directory/save/')  # re-load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "21%10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
