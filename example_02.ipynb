{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参考\n",
    "https://pytorch.apachecn.org/docs/1.2/beginner/text_sentiment_ngrams_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer,BertForSequenceClassification,BertConfig,AdamW\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "from pyhanlp import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_pre_model='./model/'                    #词表\n",
    "# bert_config='C:/Users/Administrator/Downloads/chinese_L-12_H-768_A-12/bert_config.json'       #配置文件    \n",
    "vocab_path = './model/vocab1.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据集\n",
    "train_dataset = 'data/train.csv'\n",
    "test_dataset = 'data/dev_id.csv'\n",
    "        \n",
    "def DataPrepare(file_name):\n",
    "    data = pd.read_csv(file_name)\n",
    "    data1 = pd.concat([data.question1,data.category],axis=1,keys=['question','label'])\n",
    "    data2 = pd.concat([data.question2,data.category],axis=1,keys=['question','label'])\n",
    "    data = pd.concat([data1,data2],axis=0,ignore_index=True)\n",
    "    def labels_ch(x):\n",
    "        if x == 'hypertension':\n",
    "            x = 0\n",
    "        if x == 'hepatitis':\n",
    "            x = 1\n",
    "        if x =='breast_cancer':\n",
    "            x = 2\n",
    "        if x=='aids':\n",
    "            x = 3\n",
    "        if x =='diabetes':\n",
    "            x = 4\n",
    "        return x\n",
    "    data.label = [labels_ch(la) for la in data.label]\n",
    "    return data\n",
    "\n",
    "train_dataset = DataPrepare(train_dataset)\n",
    "test_dataset = DataPrepare(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "E:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# 分词\n",
    "def cut_sentents(sentents):\n",
    "    HanLP.Config.ShowTermNature = False #不显示词性\n",
    "    PerceptronSegmenter= JClass('com.hankcs.hanlp.model.perceptron.PerceptronSegmenter')\n",
    "    segment = PerceptronSegmenter()\n",
    "    # segment.enablePartOfSpeechTagging(True)   # 激活数字和英文识别\n",
    "    for i,sen in enumerate(sentents):\n",
    "        sentents[i] = segment.segment(sen)\n",
    "        sentents[i] = list(sentents[i])\n",
    "#         sentents[i].insert(0, '[CLS]')\n",
    "#         sentents[i].append('[SEP]')\n",
    "    return sentents\n",
    "\n",
    "train_tokenized = cut_sentents(train_dataset.question)\n",
    "test_tokenized = cut_sentents(test_dataset.question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['艾滋病', '窗口期', '会', '出现', '腹泻', '症状', '吗']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokenized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把当前数据中分好的词添加到 BERT词表中\n",
    "sub_voc = []                      \n",
    "for sen in train_tokenized:\n",
    "    for voc in sen:\n",
    "        if voc not in sub_voc: \n",
    "            sub_voc.append(voc)\n",
    "# print(len(sub_voc))  # 7558               \n",
    "for sen in test_tokenized:\n",
    "    for voc in sen:\n",
    "        if voc not in sub_voc: \n",
    "            sub_voc.append(voc)\n",
    "# print(len(sub_voc))  # 9642\n",
    "# sub_voc # 一维list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取bert词表\n",
    "vocab = pd.read_csv(vocab_path,sep='\\t',header=None, quoting=csv.QUOTE_NONE)\n",
    "vocab = vocab.values.tolist()\n",
    "\n",
    "# 把当前数据中分好的词添加到 BERT词表中\n",
    "for voc in sub_voc:\n",
    "    if voc not in vocab: \n",
    "        vocab.insert(106, [voc])\n",
    "# type(vocab) # pandas.core.frame.DataFrame\n",
    "# vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9747, 9746, 10475, 9744, 9743, 9742, 11050]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(bert_pre_model,unk_token='[UNK]')\n",
    "input_ids = tokenizer.encode(train_tokenized[0])  \n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = pd.DataFrame(vocab)\n",
    "vocab.to_csv(\"C:/Users/Administrator/Downloads/chinese_L-12_H-768_A-12/vocab.txt\",index=False,header=False,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_seq_length:20(max_seq_length训练集最大值51),当前修改为20\n",
      "max_seq_length:20(max_seq_length训练集最大值51),当前修改为20\n"
     ]
    }
   ],
   "source": [
    "def data2feature(data):\n",
    "    # Load pretrained model/tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained(bert_pre_model,unk_token='[UNK]')\n",
    "\n",
    "    # Encode text\n",
    "    input_ids = {}\n",
    "    max_seq_length = 0\n",
    "    for i,sen in enumerate(data):\n",
    "        # Tokenized input,添加[CLS], [SEP]，并将 token 转为 vocabulary 索引\n",
    "        input_ids[i] = tokenizer.encode(sen, add_special_tokens=True,max_length=20)  \n",
    "        if max_seq_length < len(input_ids[i]): max_seq_length = len(input_ids[i])\n",
    "    # unsqueeze(0)   \n",
    "    print(\"max_seq_length:\"+str(max_seq_length)+\"(max_seq_length训练集最大值51),当前修改为20\")\n",
    "    max_seq_length = 20\n",
    "    # Zero-pad up to the sequence length.\n",
    "    def padding(input_ids,max_seq_length):\n",
    "        while len(input_ids) < max_seq_length:\n",
    "            input_ids.append(0)\n",
    "            #  input_mask.append(0)\n",
    "            #  segment_ids.append(0)\n",
    "        return input_ids\n",
    "    input = []\n",
    "    for i,_ in enumerate(input_ids):\n",
    "        input_ids[i] = padding(input_ids[i],max_seq_length)\n",
    "        input.append(input_ids[i])\n",
    "    input_ids = torch.LongTensor(input)\n",
    "    return input_ids \n",
    "\n",
    "#训练集\n",
    "train_inputs = data2feature(train_tokenized)[0:36]\n",
    "train_labels = torch.tensor(train_dataset.label).reshape(-1,1)[0:36]  # Batch size 1\n",
    "\n",
    "#测试集\n",
    "test_inputs = data2feature(test_tokenized)[0:36]\n",
    "test_labels = torch.tensor(test_dataset.label).reshape(-1,1)[0:36]  # Batch size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  9747,  9746, 10475,  9744,  9743,  9742, 11050,   102,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成dataloader\n",
    "batch_size = 5\n",
    "train_data = TensorDataset(train_inputs, train_labels)\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
    "test_data = TensorDataset(test_inputs, test_labels)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "# print(\"train_inputs.shape()\",train_inputs.shape)  # train_inputs.shape() torch.Size([3, 20])\n",
    "# print(\"train_labels.shape()\",train_labels.shape)  # train_labels.shape() torch.Size([3, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = BertForSequenceClassification.from_pretrained('bert-base-chinese',num_labels=5).to(device)\n",
    "# modelConfig = BertConfig.from_pretrained('bert-base-uncased/bert_config.json')\n",
    "model = BertForSequenceClassification.from_pretrained(bert_pre_model,num_labels=5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "outputs = model(train_inputs, labels=train_labels)\n",
    "loss, logits = outputs[:5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3311, -0.6933, -1.0277, -0.4949,  2.7394],\n",
       "        [-0.0177, -0.9140, -1.0456, -0.4949,  2.9560],\n",
       "        [ 0.1435, -0.8184, -0.8906, -0.6996,  2.9112],\n",
       "        [ 0.0705, -0.7222, -1.1606, -0.7454,  3.0610],\n",
       "        [ 0.3933, -0.3892, -0.9325, -0.5016,  2.5982],\n",
       "        [ 0.1312, -0.6197, -1.2588, -0.5065,  2.7341],\n",
       "        [ 0.1775, -0.4645, -1.2231, -0.5314,  2.5631],\n",
       "        [-0.1035, -0.7987, -1.1863, -0.7085,  3.0671],\n",
       "        [ 0.1237, -0.5449, -1.2022, -0.6488,  2.6694],\n",
       "        [ 0.1547, -0.6624, -1.0702, -0.3292,  2.6155],\n",
       "        [ 0.1051, -0.8972, -1.1437, -0.5451,  2.9720],\n",
       "        [ 0.2926, -0.7395, -1.1611, -0.3686,  2.8957],\n",
       "        [ 0.3396, -0.4224, -1.0556, -0.1200,  2.3630],\n",
       "        [ 0.4811, -0.6804, -1.3525, -0.0648,  2.3938],\n",
       "        [ 0.1913, -0.7646, -1.0654, -0.6881,  3.0653],\n",
       "        [-0.0158, -0.7981, -1.1890, -0.6760,  3.2062],\n",
       "        [ 0.2654, -0.5463, -1.0700, -0.5062,  2.6519],\n",
       "        [ 0.1812, -0.8180, -1.1129, -0.5079,  2.9467],\n",
       "        [ 0.1527, -0.7708, -1.1613, -0.6866,  2.8598],\n",
       "        [ 0.1463, -0.8780, -1.1695, -0.4905,  2.9027],\n",
       "        [ 0.0242, -0.7641, -1.0996, -0.5892,  3.0070],\n",
       "        [ 0.0584, -0.8273, -1.0240, -0.6831,  3.0733],\n",
       "        [-0.0044, -0.7035, -1.0728, -0.6000,  3.0059],\n",
       "        [ 0.2322, -0.6554, -1.1701, -0.7923,  3.1723],\n",
       "        [ 0.3326, -0.5847, -1.2526, -0.4363,  2.5808],\n",
       "        [ 0.1729, -1.0311, -1.0190, -0.6160,  3.1174],\n",
       "        [ 0.3383, -0.5885, -1.1867, -0.2915,  2.5548],\n",
       "        [ 0.0827, -0.9068, -0.9751, -0.7286,  3.1544],\n",
       "        [ 0.5974, -0.6133, -1.2071, -0.1729,  2.4829],\n",
       "        [ 0.3333, -0.7137, -1.2483, -0.4389,  2.4725],\n",
       "        [ 0.5849, -0.1937, -1.2155,  0.0183,  1.8380],\n",
       "        [ 0.1498, -0.7315, -1.0837, -0.5688,  2.8372],\n",
       "        [ 0.4382, -0.4981, -1.0891, -0.3980,  1.9129],\n",
       "        [ 0.3326, -0.5847, -1.2526, -0.4363,  2.5808],\n",
       "        [ 0.0550, -0.7049, -1.1659, -0.6098,  2.8720],\n",
       "        [ 0.3412, -0.6031, -1.1156, -0.4792,  2.6642]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(params=optimizer_grouped_parameters, \n",
    "                              lr=2e-5,correct_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义一个计算准确率的函数\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Train loss: 1.2226341888308525\n",
      "Test Accuracy: 0.175\n",
      "Epoch: 100\n",
      "Train loss: 0.47331657295580953\n",
      "Test Accuracy: 0.45\n",
      "Epoch: 200\n",
      "Train loss: 0.5597378773745731\n",
      "Test Accuracy: 0.45\n",
      "Epoch: 300\n",
      "Train loss: 0.2027156749754795\n",
      "Test Accuracy: 0.625\n",
      "Epoch: 400\n",
      "Train loss: 0.3558836270676693\n",
      "Test Accuracy: 0.45\n",
      "Epoch: 500\n",
      "Train loss: 0.21284987591207027\n",
      "Test Accuracy: 0.625\n",
      "Epoch: 600\n",
      "Train loss: 0.08749916453234619\n",
      "Test Accuracy: 0.675\n",
      "Epoch: 700\n",
      "Train loss: 0.08543706410273444\n",
      "Test Accuracy: 0.675\n",
      "Epoch: 800\n",
      "Train loss: 0.08797759396838956\n",
      "Test Accuracy: 0.675\n",
      "Epoch: 900\n",
      "Train loss: 0.08598874708695803\n",
      "Test Accuracy: 0.625\n",
      "Epoch: 1000\n",
      "Train loss: 0.09064131304512557\n",
      "Test Accuracy: 0.625\n",
      "Epoch: 1100\n",
      "Train loss: 0.07875517905631568\n",
      "Test Accuracy: 0.675\n",
      "Epoch: 1200\n",
      "Train loss: 0.4921140979663505\n",
      "Test Accuracy: 0.45\n",
      "Epoch: 1300\n",
      "Train loss: 0.48169442109792726\n",
      "Test Accuracy: 0.45\n",
      "Epoch: 1400\n",
      "Train loss: 0.4672973936685594\n",
      "Test Accuracy: 0.45\n",
      "Epoch: 1500\n",
      "Train loss: 0.5014086491255512\n",
      "Test Accuracy: 0.45\n",
      "Epoch: 1600\n",
      "Train loss: 0.47061381186313156\n",
      "Test Accuracy: 0.45\n",
      "Epoch: 1700\n",
      "Train loss: 0.4725168987752113\n",
      "Test Accuracy: 0.45\n",
      "Epoch: 1800\n",
      "Train loss: 0.5025168023746573\n",
      "Test Accuracy: 0.45\n",
      "Epoch: 1900\n",
      "Train loss: 0.43990749025397236\n",
      "Test Accuracy: 0.45\n"
     ]
    }
   ],
   "source": [
    "#训练开始\n",
    "train_loss_set = []#可以将loss加入到列表中，后期画图使用\n",
    "best_accuracy = 0\n",
    "epochs = 2000\n",
    "for epoch in range(epochs):\n",
    "    if(epoch%100 == 0): print(\"Epoch: {}\".format(epoch))\n",
    "    #训练开始\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        #取第一个位置，BertForSequenceClassification第一个位置是Loss，第二个位置是[CLS]的logits\n",
    "        loss = model(b_input_ids, token_type_ids=None, labels=b_labels)[0]\n",
    "        train_loss_set.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "    if(epoch%100 == 0):  print(\"Train loss: {}\".format(tr_loss / nb_tr_steps))\n",
    "    #模型评估\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    for batch in test_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, token_type_ids=None)[0]\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += 1\n",
    "    accuracy = eval_accuracy / nb_eval_steps\n",
    "    if(best_accuracy<accuracy):\n",
    "        best_accuracy = accuracy\n",
    "        model.save_pretrained('./directory/save/')  # 保存最高的accuracy\n",
    "        # model = model_class.from_pretrained('./directory/save/')  # re-load\n",
    "    if(epoch%100 == 0):  print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7999999999999999\n"
     ]
    }
   ],
   "source": [
    "# 加载已保存的准确率最高的模型\n",
    "model = BertForSequenceClassification.from_pretrained('./directory/save/')  # re-load\n",
    "# logits = model(test_inputs)\n",
    "#模型评估\n",
    "model.eval()\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "for batch in test_dataloader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_labels = batch\n",
    "    with torch.no_grad():\n",
    "        logits = model(b_input_ids, token_type_ids=None)[0]\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "    nb_eval_steps += 1\n",
    "accuracy = eval_accuracy / nb_eval_steps\n",
    "if(best_accuracy<accuracy):\n",
    "    best_accuracy = accuracy\n",
    "    model.save_pretrained('./directory/save/')  # 保存最高的accuracy\n",
    "    # model = model_class.from_pretrained('./directory/save/')  # re-load\n",
    "#if(epoch%100 == 0):  \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
